{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO++bxwG2tr6DfxbibSKs8/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/saurabhgupta18891/LLM_Experiments/blob/main/QA_using_GPT_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HwVzjtHGjt8O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "250998b7-815b-4fcc-d259-b69495e6babc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "    \"id\": \"cmpl-77kbiqY4dyxVNwvm5FcUPD72xWaj8\",\n",
            "    \"object\": \"text_completion\",\n",
            "    \"created\": 1682081810,\n",
            "    \"model\": \"text-davinci-002\",\n",
            "    \"choices\": [\n",
            "        {\n",
            "            \"text\": \"\\n\\nYour name is Saurabh.\",\n",
            "            \"index\": 0,\n",
            "            \"logprobs\": null,\n",
            "            \"finish_reason\": \"stop\"\n",
            "        }\n",
            "    ],\n",
            "    \"usage\": {\n",
            "        \"prompt_tokens\": 22,\n",
            "        \"completion_tokens\": 9,\n",
            "        \"total_tokens\": 31\n",
            "    }\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "import openai\n",
        "import json\n",
        "\n",
        "# Authenticate with the OpenAI API\n",
        "\n",
        "# Define the prompt for the question-answering task\n",
        "prompt = (\"\"\"My name is saurabh\"\"\"\n",
        "          \"\"\"Please answer the following question:\\n\"\"\"\n",
        "          #\"\"\"What is the capital of France?\\n\"\"\"\n",
        "          \"\"\"What is my name?\\n\"\"\"\n",
        "          \"\"\"Answer:\"\"\")\n",
        "\n",
        "# Define the training data for the model\n",
        "training_data = [\n",
        "    {\"text\": \"The capital of France is Paris.\", \"label\": \"Paris\"},\n",
        "    {\"text\": \"France's capital is Paris.\", \"label\": \"Paris\"},\n",
        "    {\"text\": \"The capital of France is Berlin.\", \"label\": \"Berlin\"},\n",
        "    {\"text\": \"The capital of France is London.\", \"label\": \"London\"},\n",
        "    {\"text\": \"The capital of France is Madrid.\", \"label\": \"Madrid\"}\n",
        "]\n",
        "\n",
        "# Train the model on the training data\n",
        "response = openai.Completion.create(\n",
        "    model=\"text-davinci-002\",\n",
        "    prompt=prompt,\n",
        "    #examples=training_data,\n",
        "    temperature=0.7,\n",
        "    max_tokens=60,\n",
        "    n=1,\n",
        "    stop=None,\n",
        "    frequency_penalty=0,\n",
        "    presence_penalty=0\n",
        ")\n",
        "\n",
        "# Print the response from the model\n",
        "print(json.dumps(response, indent=4))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "examples=[\n",
        "{\n",
        "\"input\": \"A toothpaste that whitens your teeth and freshens your breath.\",\n",
        "\"output\": \"Sparkle and shine with every smile.\"\n",
        "},\n",
        "{\n",
        "\"input\": \"A shampoo that nourishes your hair and prevents dandruff.\",\n",
        "\"output\": \"Healthy hair starts with Head & Shoulders.\"\n",
        "}\n",
        "],"
      ],
      "metadata": {
        "id": "xAQ8ffImraYq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(response.choices[0].text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0bO02TS-pzXJ",
        "outputId": "95379c79-b5e1-4802-a197-9880e6c2f9ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "The capital of France is Paris.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response1 = openai.Completion.create(\n",
        "engine=\"text-davinci-003\",\n",
        "prompt=\"Write a catchy slogan for a product.\\n\",\n",
        "examples=[\n",
        "{\n",
        "\"input\": \"A toothpaste that whitens your teeth and freshens your breath.\",\n",
        "\"output\": \"Sparkle and shine with every smile.\"\n",
        "},\n",
        "{\n",
        "\"input\": \"A shampoo that nourishes your hair and prevents dandruff.\",\n",
        "\"output\": \"Healthy hair starts with Head & Shoulders.\"\n",
        "}\n",
        "],\n",
        "max_tokens=10,\n",
        "temperature=0.5,\n",
        "stop=\"\\n\"\n",
        ")\n",
        "\n",
        "print(response1[\"choices\"][0][\"text\"])"
      ],
      "metadata": {
        "id": "nDrxrxGjtFMY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AfTwwXlfl7gf",
        "outputId": "7ab8ad2c-aa0c-47b4-bc96-92711dc5520e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting openai\n",
            "  Downloading openai-0.27.4-py3-none-any.whl (70 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.3/70.3 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.20 in /usr/local/lib/python3.9/dist-packages (from openai) (2.27.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from openai) (4.65.0)\n",
            "Collecting aiohttp\n",
            "  Downloading aiohttp-3.8.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m25.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests>=2.20->openai) (2.0.12)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.20->openai) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests>=2.20->openai) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests>=2.20->openai) (2022.12.7)\n",
            "Collecting async-timeout<5.0,>=4.0.0a3\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Collecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.8.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (264 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m264.6/264.6 kB\u001b[0m \u001b[31m26.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiosignal>=1.1.2\n",
            "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->openai) (23.1.0)\n",
            "Collecting frozenlist>=1.1.1\n",
            "  Downloading frozenlist-1.3.3-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (158 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.8/158.8 kB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-6.0.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.2/114.2 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: multidict, frozenlist, async-timeout, yarl, aiosignal, aiohttp, openai\n",
            "Successfully installed aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 frozenlist-1.3.3 multidict-6.0.4 openai-0.27.4 yarl-1.8.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import ast  # for converting embeddings saved as strings back to arrays\n",
        "import openai  # for calling the OpenAI API\n",
        "import pandas as pd  # for storing text and embeddings data\n",
        "import tiktoken  # for counting tokens\n",
        "from scipy import spatial  # for calculating vector similarities for search\n",
        "\n",
        "\n",
        "# models\n",
        "EMBEDDING_MODEL = \"text-embedding-ada-002\"\n",
        "GPT_MODEL = \"gpt-3.5-turbo\""
      ],
      "metadata": {
        "id": "IVYzeOJkl-Ed"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tiktoken"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VYnR8a1ix5Jq",
        "outputId": "5a8dd7ed-0c26-4a55-d4b4-49b4a636b66e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.3.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m31.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.9/dist-packages (from tiktoken) (2022.10.31)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.9/dist-packages (from tiktoken) (2.27.1)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests>=2.26.0->tiktoken) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests>=2.26.0->tiktoken) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests>=2.26.0->tiktoken) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.26.0->tiktoken) (1.26.15)\n",
            "Installing collected packages: tiktoken\n",
            "Successfully installed tiktoken-0.3.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = 'Which athletes won the gold medal in curling at the 2022 Winter Olympics?'\n",
        "\n",
        "response = openai.ChatCompletion.create(\n",
        "    messages=[\n",
        "        {'role': 'system', 'content': 'You answer questions about the 2022 Winter Olympics.'},\n",
        "        {'role': 'user', 'content': query},\n",
        "    ],\n",
        "    model=GPT_MODEL,\n",
        "    temperature=0,\n",
        ")\n",
        "\n",
        "print(response['choices'][0]['message']['content'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DLj5wOonyLnn",
        "outputId": "8551a279-aa97-43d1-f9f5-69e15841c4d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I'm sorry, but as an AI language model, I don't have information about the future events. The 2022 Winter Olympics will be held in Beijing, China from February 4 to 20, 2022. The curling events will take place during the games, and the winners of the gold medal in curling will be determined at that time.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "article=\"\"\"my name is saurabh gupta\"\"\""
      ],
      "metadata": {
        "id": "SbnlkW3tzZJ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = f\"\"\"Use the below article to answer the subsequent question. If the answer cannot be found, write \"I don't know.\"\n",
        "\n",
        "Article:\n",
        "\\\"\\\"\\\"\n",
        "{article}\n",
        "\\\"\\\"\\\"\n",
        "\n",
        "Question: What is my name?\"\"\"\n",
        "\n",
        "response = openai.ChatCompletion.create(\n",
        "    messages=[\n",
        "        {'role': 'system', 'content': 'You answer questions about the article'},\n",
        "        {'role': 'user', 'content': query},\n",
        "    ],\n",
        "    model=GPT_MODEL,\n",
        "    temperature=0,\n",
        ")\n",
        "\n",
        "print(response['choices'][0]['message']['content'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rZJks8czyy8B",
        "outputId": "48ee6912-de09-480f-9169-cac29982eb54"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your name is Saurabh Gupta.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Collect a set of questions and answers that are relevant to your domain or topic of interest.\n",
        "\n",
        "# Save the questions and answers in a text file, with each question and its corresponding answer separated by a newline character.\n",
        "\n",
        "# Use the OpenAI API client to interact with the GPT-3 API. You will need to authenticate with your API key to access the API.\n",
        "\n",
        "# Use the GPT-3 API to fine-tune the model on your dataset. You will need to use the davinci or curie model, as these are the models that support fine-tuning.\n",
        "\n",
        "# Define the parameters for fine-tuning, such as the number of epochs, the learning rate, and the batch size. You can experiment with different values to find the optimal configuration for your use case.\n",
        "\n",
        "# Once the model has been fine-tuned, you can generate answers to questions by providing the question as input to the model and receiving the output as the answer."
      ],
      "metadata": {
        "id": "SHOflRB_2uAU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "import json\n",
        "\n",
        "# set your OpenAI API key\n",
        "openai.api_key = \"your_api_key\"\n",
        "\n",
        "# create your custom question-answer pairs\n",
        "dataset = {\n",
        "  \"data\": [\n",
        "    {\n",
        "      \"question\": \"What is the capital of France?\",\n",
        "      \"answer\": \"Paris\"\n",
        "    },\n",
        "    {\n",
        "      \"question\": \"What is the largest planet in our solar system?\",\n",
        "      \"answer\": \"Jupiter\"\n",
        "    },\n",
        "    {\n",
        "      \"question\": \"Who wrote The Great Gatsby?\",\n",
        "      \"answer\": \"F. Scott Fitzgerald\"\n",
        "    }\n",
        "  ]\n",
        "}\n",
        "\n",
        "# save the dataset to a JSON file\n",
        "with open(\"dataset.json\", \"w\") as f:\n",
        "    json.dump(dataset, f)\n",
        "\n",
        "# fine-tune the GPT-3 model on your dataset\n",
        "model = \"text-davinci-002\"\n",
        "prompt = \"Q: What is the capital of France?\\nA: Paris\\nQ: What is the largest planet in our solar system?\\nA: Jupiter\\nQ: Who wrote The Great Gatsby?\\nA: F. Scott Fitzgerald\"\n",
        "fine_tuned_model = openai.Completion.create(engine=model, prompt=prompt, max_tokens=1024, n=1,stop=None,temperature=0.5)\n",
        "\n",
        "# save the fine-tuned model to a file\n",
        "with open(\"fine_tuned_model.json\", \"w\") as f:\n",
        "    json.dump(fine_tuned_model, f)\n"
      ],
      "metadata": {
        "id": "W8QwDKL5x0X5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "import pandas as pd\n",
        "\n",
        "# Authenticate with the OpenAI API\n",
        "openai.api_key = \"your_api_key\"\n",
        "\n",
        "# Load your data set\n",
        "df = pd.read_csv(\"your_data.csv\")\n",
        "\n",
        "# Fine-tune the GPT-3 model on your data set\n",
        "response = openai.Completion.create(\n",
        "  engine=\"davinci\",\n",
        "  prompt=\"Question: \" + df['question'] + \"\\nAnswer: \" + df['answer'],\n",
        "  temperature=0.5,\n",
        "  max_tokens=1024,\n",
        "  n=1,\n",
        "  stop=None,\n",
        "  frequency_penalty=0,\n",
        "  presence_penalty=0\n",
        ")\n",
        "\n",
        "# Store the fine-tuned model in a file for future use\n",
        "with open(\"fine_tuned_model.txt\", \"w\") as f:\n",
        "  f.write(response.choices[0].text)\n"
      ],
      "metadata": {
        "id": "DyeKskS8w6VD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "import json\n",
        "\n",
        "# Set up OpenAI API client\n",
        "openai.api_key = \"YOUR_API_KEY\"\n",
        "\n",
        "# Define dataset\n",
        "dataset = {\n",
        "    \"questions\": [\n",
        "        \"What is the capital of France?\",\n",
        "        \"What is the largest mammal in the world?\",\n",
        "        \"Who wrote the Harry Potter series?\"\n",
        "    ],\n",
        "    \"answers\": [\n",
        "        \"Paris\",\n",
        "        \"Blue whale\",\n",
        "        \"J.K. Rowling\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "# Convert dataset to JSON format\n",
        "dataset_json = json.dumps(dataset)\n",
        "\n",
        "# Fine-tune GPT-3 model on dataset\n",
        "response = openai.Completion.create(\n",
        "    engine=\"davinci\",\n",
        "    prompt=dataset_json,\n",
        "    max_tokens=1024,\n",
        "    n=1,\n",
        "    stop=None,\n",
        "    temperature=0.7,\n",
        ")\n",
        "\n",
        "# Retrieve fine-tuned GPT-3 model\n",
        "model = response.choices[0].text\n",
        "\n",
        "# Test fine-tuned model\n",
        "question = \"What is the capital of Italy?\"\n",
        "response = openai.Completion.create(\n",
        "    engine=\"davinci\",\n",
        "    prompt=model + question,\n",
        "    max_tokens=1024,\n",
        "    n=1,\n",
        "    stop=None,\n",
        "    temperature=0.7,\n",
        ")\n",
        "\n",
        "# Print answer\n",
        "print(response.choices[0].text)\n"
      ],
      "metadata": {
        "id": "2dGZPVaZy5r9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "import os\n",
        "\n",
        "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "# Define your dataset\n",
        "data = [\n",
        "    \"What is the capital of France?\\nParis\",\n",
        "    \"What is the largest planet in the solar system?\\nJupiter\",\n",
        "    \"What is the boiling point of water?\\n100 degrees Celsius\"\n",
        "]\n",
        "\n",
        "# Save the dataset to a file\n",
        "with open(\"qa_dataset.txt\", \"w\") as f:\n",
        "    for item in data:\n",
        "        f.write(\"%s\\n\" % item)\n",
        "\n",
        "# Fine-tune the GPT-3 model on the dataset\n",
        "model_engine = \"davinci\" # or \"curie\"\n",
        "prompt = \"Answer: \"\n",
        "temperature = 0.5\n",
        "max_tokens = 100\n",
        "epochs = 5\n",
        "batch_size = 32\n",
        "\n",
        "response = openai.Completion.create(\n",
        "    engine=model_engine,\n",
        "    prompt=prompt,\n",
        "    temperature=temperature,\n",
        "    max_tokens=max_tokens,\n",
        "    n_epochs=epochs,\n",
        "    batch_size=batch_size,\n",
        "    dataset=f\"file://qa_dataset.txt\",\n",
        ")\n",
        "\n",
        "# Generate answers to questions\n",
        "question = \"What is the capital of Spain?\"\n",
        "response = openai.Completion.create(\n",
        "    engine=model_engine,\n",
        "    prompt=f\"Question: {question}\\nAnswer:\",\n",
        "    temperature=temperature,\n",
        "    max_tokens=max_tokens,\n",
        ")\n",
        "\n",
        "print(response.choices[0].text)\n"
      ],
      "metadata": {
        "id": "XPrco4eu2alv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "import os\n",
        "\n",
        "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "# Define your dataset\n",
        "data = [\n",
        "    \"What is the capital of France?\\nParis\",\n",
        "    \"What is the largest planet in the solar system?\\nJupiter\",\n",
        "    \"What is the boiling point of water?\\n100 degrees Celsius\"\n",
        "]\n",
        "\n",
        "# Save the dataset to a file\n",
        "with open(\"qa_dataset.txt\", \"w\") as f:\n",
        "    for item in data:\n",
        "        f.write(\"%s\\n\" % item)\n",
        "\n",
        "# Fine-tune the GPT-3 model on the dataset\n",
        "model_engine = \"davinci\" # or \"curie\"\n",
        "epochs = 5\n",
        "learning_rate = 1e-5\n",
        "batch_size = 32\n",
        "model_name = \"QA Custom Model\"\n",
        "\n",
        "fine_tune_params = {\n",
        "    \"model\": f\"{model_engine}-fine-tuned-{model_name}\",\n",
        "    \"prompt\": f\"Question: \",\n",
        "    \"temperature\": 0.5,\n",
        "    \"max_tokens\": 1024,\n",
        "    \"n_epochs\": epochs,\n",
        "    \"learning_rate\": learning_rate,\n",
        "    \"batch_size\": batch_size,\n",
        "    \"dataset\": f\"file://qa_dataset.txt\"\n",
        "}\n",
        "\n",
        "fine_tuned_model = openai.Model.create(fine_tune_params)\n",
        "\n",
        "# Test the fine-tuned model with sample questions\n",
        "test_questions = [\"What is the capital of Spain?\", \"What is the boiling point of alcohol?\"]\n",
        "for question in test_questions:\n",
        "    prompt = f\"Question: {question}\\nAnswer:\"\n",
        "    response = openai.Completion.create(\n",
        "        engine=model_engine,\n",
        "        prompt=prompt,\n",
        "        max_tokens=1024,\n",
        "        n=1,\n",
        "        temperature=0.5,\n",
        "        model=f\"{model_engine}-fine-tuned-{model_name}\"\n",
        "    )\n",
        "    print(f\"Q: {question}\")\n",
        "    print(f\"A: {response.choices[0].text.strip()}\\n\")\n"
      ],
      "metadata": {
        "id": "wI1W5vV-30Fw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "import os\n",
        "\n",
        "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "# Define your dataset\n",
        "data = [\n",
        "    {\"question\": \"What is the capital of France?\", \"answer\": \"Paris\"},\n",
        "    {\"question\": \"What is the largest planet in the solar system?\", \"answer\": \"Jupiter\"},\n",
        "    {\"question\": \"What is the boiling point of water?\", \"answer\": \"100 degrees Celsius\"}\n",
        "]\n",
        "\n",
        "# Fine-tune the GPT-3 model on the dataset\n",
        "model_engine = \"davinci\" # or \"curie\"\n",
        "prompt = \"Answer: \"\n",
        "temperature = 0.5\n",
        "max_tokens = 100\n",
        "epochs = 5\n",
        "batch_size = 32\n",
        "\n",
        "# Create a list of questions and answers for training\n",
        "questions = [item['question'] for item in data]\n",
        "answers = [item['answer'] for item in data]\n",
        "\n",
        "# Concatenate the questions and answers with a delimiter\n",
        "delimiter = '\\n'\n",
        "train_text = delimiter.join([f\"Question: {q}\\nAnswer: {a}\" for q, a in zip(questions, answers)])\n",
        "\n",
        "# Fine-tune the GPT-3 model on the training data\n",
        "response = openai.Completion.create(\n",
        "    engine=model_engine,\n",
        "    prompt=prompt,\n",
        "    temperature=temperature,\n",
        "    max_tokens=max_tokens,\n",
        "    n_epochs=epochs,\n",
        "    batch_size=batch_size,\n",
        "    dataset=f\"{train_text}\"\n",
        ")\n",
        "\n",
        "# Generate answers to questions\n",
        "question = \"What is the capital of Spain?\"\n",
        "response = openai.Completion.create(\n",
        "    engine=model_engine,\n",
        "    prompt=f\"Question: {question}\\nAnswer:\",\n",
        "    temperature=temperature,\n",
        "    max_tokens=max_tokens,\n",
        ")\n",
        "\n",
        "print(response.choices[0].text)\n"
      ],
      "metadata": {
        "id": "OvQ0D7ic4w_C"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}